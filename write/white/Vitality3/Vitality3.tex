\documentclass[11pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{booktabs}

\newcommand{\Ecal}{\mathcal{E}}
\newcommand{\Pcal}{\mathcal{P}}
\newcommand{\Xspace}{\mathcal{X}}
\newcommand{\Zspace}{\mathcal{Z}}
\newcommand{\Ospace}{\mathcal{O}}
\newcommand{\Sset}{\mathcal{S}}

\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}

\title{Vitality as the Mechanization of Play:\\The Dual Capacities of Dynamic Engagement\\and Symbolic Abstraction\\(Version 3)}
\date{}
\author{}

\begin{document}
\maketitle

\begin{center}
\textit{``The creation of something new is not accomplished by the intellect but by the play instinct acting from inner necessity. The creative mind plays with the objects it loves.'' --- C.G.\ Jung}
\end{center}

\bigskip

\begin{abstract}
We propose Vitality as a formal theory of autonomous development grounded in play.
Play---across species and developmental stages---exhibits two complementary capacities: \textit{dynamic bidirectional engagement} (oscillating between affecting and being affected) and \textit{symbolic abstraction} (operating on extracted features and invariants of a compositional nature rather than raw percepts).
We formalize the first as Empowerment--Plasticity oscillation (Abel et al.\ 2025) and the second as algebraic constraint learning that extracts discrete, composable, transformation-invariant representations.
Their integration---bidirectional engagement generating disciplined interaction, symbolic abstraction extracting reusable structure that becomes substrate for further engagement---constitutes a developmental loop producing open-ended growth without external rewards.
We present the best available mechanisms for each capacity, then subject them to two structural critiques: entropic objectives cannot be the highest-level goal for autonomy, and the missing symbolic layer creates a self-reinforcing trap with entropy-based methods.
These critiques are not objections to Vitality---they define the research program it points toward.
\end{abstract}

%% ----------------------------------------------------------------
\section{Play}

\subsection{Introduction}

The term ``vitality'' is not our invention. Stern (2010) introduced \textit{vitality affects} and later \textit{forms of vitality} as the dynamic, felt qualities of experience---the surging, fading, explosive, or gentle contours that accompany all human action and perception. Trevarthen identified an \textit{intrinsic motive pulse}---an innate rhythmic organization of intention and feeling that drives communicative engagement from birth (Trevarthen 1999). As Trevarthen puts it: ``play is the regulation of vitality'' (Trevarthen, in Delafield-Butt \& Reddy 2025). We take this seriously and literally.

Play appears across species wherever nervous systems support flexible behavior. Piaget demonstrated that play is central to cognitive construction; Trevarthen showed that intersubjective play founds social cognition, explicitly characterizing it through vitality affects and vitality dynamics---the reciprocal, temporally structured emotional exchanges between infant and caregiver that scaffold all later development; Panksepp identified dedicated neural circuitry for play that cannot be reduced to other motivational systems. Play has structure---it is not random exploration, but rather a specific behavioral regime characterized by two complementary capacities.

This is not merely a metaphor. When we say play is the mother of all values, we mean it mechanistically: play exhibits a specific architecture---two complementary capacities in a self-sustaining loop---that generates open-ended development without external reward signals. What follows after a brief overview of our synthetic multidisciplinary approach to going beyond optimization methods to developmental mechanism is a mechanization of play---a formal account of its two capacities and the architecture they generate.

\subsection{Methodology}

Our approach is synthetic and multidisciplinary: we bring together developmental psychology, developmental robotics, and formal mathematics not as separate contributions but as mutually constraining disciplines. Accurate descriptions of how organisms develop guide the construction of artificial agents---and the successes and failures of those constructions test the descriptions. We are, in this regard, not unlike the alchemists playing with their mercury to transmute it into gold, or the pirate on the high seas in search of the fountain of youth. We differ only in the material of our play, which happens to be modern information-theoretic technology. We begin with developmental psychology---and it is not lost on us that \emph{psyche} is the Greek for soul. The search for what makes the living \emph{live}, the causal and eleatic soul of the organism, is the oldest question psychology ever asked (Roachford 2023). We are simply resuming it with better tools. This creative scientific style is visualized in Figure~\ref{fig:describe-build}.

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{figures/describe_build.png}
\caption{Developmental psychology describes how infants explore, grow through cognitive stages, and learn social skills. Developmental robotics builds embodied agents that implement candidate learning mechanisms. Combined through play, accurate descriptions guide construction---and construction tests descriptions. We also intend to add folk psychology---including both religion and philosophy---as informing developmental psychology alongside the origin of life.}
\label{fig:describe-build}
\end{figure}

\section{Vitality: Mechanizing Play}

Play has the right structure---but structure alone is not a theory. To go from the observation that play exhibits bidirectional engagement and symbolic abstraction to a mechanistic account, we must identify the two capacities precisely, show how they close into a developmental loop, and specify the architecture that results. This section does this informally; Section~4 does it formally. The parallel is deliberate: mechanizing play (here) and formalizing play (there) are two passes over the same structure at different levels of precision.

\subsection{First Capacity: Dynamic Bidirectional Engagement}

Consider rough-and-tumble play: animals oscillate between influence and receptivity. If one player only pins (pure domination), the game ends. If one player is only pinned (pure submission), the game ends. The game persists if and only if both participants oscillate between affecting and being affected.

We formalize this as two complementary capacities. \textbf{Affecting} (empowerment) is the capacity for actions to reliably influence future observations. \textbf{Being affected} (plasticity) is the capacity for observations to reliably reshape future actions. The pathologies are instructive: pure empowerment leads to rigidity, pure plasticity to puppetry. Play lives where both capacities are simultaneously high and dynamically balanced.

The oscillatory structure is straightforward: perturb the world, observe the result, update your behavior, re-perturb. When this oscillation collapses, the developmental engine stalls.

\subsection{Second Capacity: Symbolic Abstraction}

Consider pretend play: a child picks up a stick and calls it a sword. The operation is abstract the functional role, map it onto a novel substrate, operate on the abstraction. This is symbolic computation in its most basic form: extract structural invariants from experience, recombine them in novel configurations.
The symbolic capacity couples naturally to the dynamic capacity. Active probing discovers transformation-invariant features. Once extracted, these symbols become new control targets. For example, once a child abstracts ``tool,'' they begin seeking control over tool-use at this higher level of abstraction, not just control over individual graspable objects.

\subsection{The Developmental Loop}

\begin{definition}[Play as Developmental Loop]
Play implements a self-organizing developmental cycle:
\begin{enumerate}[label=(\alph*)]
  \item \textbf{Bidirectional engagement} generates disciplined interaction near competence boundary.
  \item \textbf{Symbolic abstraction} extracts reusable, transformation-invariant, composable features.
  \item \textbf{Projection}: symbols become new E--P substrates---agent oscillates at higher abstraction level.
  \item \textbf{Repeat}.
\end{enumerate}
Result: open-ended growth without external rewards or hand-designed curricula.
\end{definition}

Symbolic abstraction is \textit{interiorization}---the world's causal structure is taken inward and consolidated as reusable representational inventory. Projection is \textit{exteriorization}---internal symbolic structure is cast back outward as new control targets, reshaping how the agent engages with the world. The developmental loop is this rhythm of interiorization and exteriorization, each feeding the other.

\subsection{The Dual-Dual Structure}

The two capacities generate a dual-dual architecture. The first duality is dialectical: E and P are mutually regulating opposites whose oscillation constitutes the developmental dynamic. The second is the dual-laws model (Nishitsunoi et al.\ 2025): feedback error is independently reducible at two levels: by changing weights or executing actions at the base level, or by changing the mathematical structure of the equations themselves---augmenting the relationships between the indices, where each index points to a learned representation---at the symbolic index level. Together these produce a natural three-layer phenomenology.

\subsection{The Triadic Structure of Phenomenal Cognition}

The dual-dual structure generates three phenomenologically distinct layers of cognition. The super-conscious layer embodies a horizontal duality---better understood as a dialectic---between empowerment and plasticity, whose oscillation modulates the vertical duality between the conscious (index) layer and the sub-conscious (dynamic) layer. Each layer has its own dynamics, its own temporality, and its own relationship to the E--P oscillator---but they are not independent modules. They are aspects of a single coupled system, distinguished by the level of abstraction at which they operate.

\subsubsection*{Super-Conscious: The E--P Dialectic}

The bidirectional engagement layer. Empowerment and plasticity co-regulate each other as a Turing-style oscillator---the pre-symbolic drive that selects what is worth attending to and generates the developmental dynamic. This is the first duality: E without P is rigidity, P without E is dissolution, and the system lives in their oscillation. In phenomenal terms this is the true self which sits behind all action and perception---the difference which makes a difference, the silent watcher, the gentle gardener and the slave driver, the seat of the soul---it lives at the level of causation. This is the mercurial hermaphrodite, the true will to power, the cosmic child, the \textit{coincidentia oppositorum}, the \textit{prima materia} and the goal of its own transformation (Jung 1967)---of every living thing. In more measured vocabulary, this is the locus of agency.

\subsubsection*{Conscious: The Symbolic Index Layer}

The symbolic index level in the dual-laws model---the level of discrete symbols that point to learned representations. Already-learned representations are \textbf{selected, composed, and recombined} here via an \textit{index sequence}---a discrete ordering of indices, each tied to a particular representational feature. Each index selects a particular structured description of the world, and the sequence determines which descriptions are active and how they combine. This selection is influenced by the E--P dynamics from above, by external environmental inputs from below via associative triggering, and potentially through social mediation as in \textbf{\textit{Cybernetic Mimesis}}. The core point is that a \textbf{competitive dynamics} lives at this layer---it decides which index of the learned perceptual features interprets perceptual input and which control objectives govern action---this is schema selection for action and perception. Modifying the index sequence is Type~2 processing: discrete, exploratory, symbolic---the agent manipulates \textit{which representations to deploy}, not how to learn or execute them. Since Type~2 is inherently discrete, it plausibly becomes advanced in organisms with sophisticated linguistic capacities---language itself being an index system. There is no reason to think Type~2 is uniquely human, but its elaboration likely tracks the capacity for discrete symbolic recombination.

\subsubsection*{Sub-Conscious: The Neuronal Dynamics Layer}

The base level. This layer changes to satisfy the constraints generated by the higher layer, either through \textbf{learning} (algebraic constraint learning lives here) or through \textbf{behavior} (feedback control and reflexes). This is Type~1 processing: continuous, goal-directed, subsymbolic. It encompasses the embodied dynamics of the agent--world coupling: motor control, proprioception, sensorimotor loops---the physics of interaction through which all higher-level structure must ultimately be realized. Crucially, through execution and reflexes this layer generates the interaction data from which new representations will be extracted, feeding back up to the conscious and super-conscious layers. The sub-conscious is both the executor and the data source---it enacts the schemas selected above and, in doing so, produces the raw material from which further schemas are learned.

\section{Autonomy and the Problem of Meaning}

\subsection{Klopf's Critique and the Coupling of Feedback}

Klopf (1982)---the forefather of reinforcement learning---built his entire thesis around positive feedback. His \textit{Hedonistic Neuron} argued that neurons are fundamentally reward-seeking: they do not merely reduce error, they pursue stimulation. Classical cybernetics---Wiener, Ashby---had built a science of homeostasis, negative feedback toward set-points. Klopf diagnosed this as a fundamental omission. Living systems are also self-amplifying: they seek stimulation, escalate challenges, pursue novelty. He called this \textit{heterostasis}, the drive toward more rather than back to baseline. Sepulchre (2022) showed from engineering that robust adaptive systems require both positive and negative feedback coupled together. Negative feedback alone converges to a fixed point. Positive feedback alone explodes. Development, learning, and play all require both, coupled.

\subsection{Autonomy: Self-Regulation and Self-Amplification}

This brings us to the notion of autonomy underlying Vitality. Autonomy is not just homeostasis, and not just heterostasis. It is both, coupled, in the same system.

\begin{definition}[Autonomy]
Autonomy = self-regulation + self-amplification. The problem with pure positive feedback---Klopf's heterostasis alone---is that it may exhaust the system, destroy the system, and ultimately undermine its own existence. Unregulated self-amplification burns through resources, destabilizes the structures it depends on, and ceases to be. This is where negative feedback is necessary: not as an externally imposed brake, but as an internally generated constraint that preserves the conditions for continued amplification. If this regulation is not provided by the organism itself, then the organism lacks autonomy---it must outsource its self-regulation to the environment. And if the amplification is unchecked, the system destroys itself. Only when both self-regulation and self-amplification are internally generated does the system become a genuine source of causation rather than a channel for external forces.
\end{definition}

This definition distinguishes autonomy from viability. Viability is purely negative feedback---self-regulation without self-amplification. A thermostat is viable. It maintains a set-point through reactive correction, but it does not generate its own movement. Viability requires external perturbation to produce behavior. An organism can be viable (persist as a coherent entity through homeostatic regulation) without being autonomous in the sense we mean here.

Genuine autonomy requires the E--P oscillator. When empowerment and plasticity are coupled as a dynamical system, that coupled system is autonomous. E regulates P (influencing the world shapes what can reconfigure you), and P regulates E (reconfigurability sustains influence in non-stationary environments). The oscillator self-regulates through this bidirectional coupling. Simultaneously, E and P mutually amplify each other: expanded empowerment generates richer feedback for plasticity, and expanded plasticity enables more sophisticated empowerment. The E--P engine generates its own developmental dynamics. It does not wait for perturbation---it produces self-generated movement.

Vitality is this autonomous regime. The E--P oscillator sustains itself through bidirectional co-regulation. $V_t$ measures the strength of this regime, but vitality itself is the pattern of E--P co-regulation, not the score. The convergence is striking. Sepulchre's control-across-scales work demands this dual structure. Klopf diagnosed its absence in cybernetics. Play provides it naturally: a regime where self-regulation and self-amplification are co-constitutive. Viability keeps you alive. Vitality makes you an autonomous agent.

\subsection{Evolution, Agency and Cybernetic Mimesis}

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{figures/Vitality_Evolution.png}
\caption{Model of Agency (Evolution). Each agent has internal structure: Vitality drives schema selection over macro--micro dynamics coupled to the environment. Evolution operates through both external selection (environmental fitness, natural selection) and internal selection (intrinsically defined functionality and pragmatics---Piaget's organic selection).}
\label{fig:vitality-evolution}
\end{figure}

This reconciles the tension between selection mechanisms and the generation of cybernetic structures highlighted by Deacon (2005). Between agents, symbolic exchange occurs through lateral gene transfer and cybernetic mimesis---social structural influences that sit alongside Darwinian selection as mechanisms for transmitting and transforming representational structure. The model highlights the subtle nuance required in separating external selection---natural selection, necessity as the mother of invention---from an internal drive---foregrounding the agential turn in evo-devo (Walsh 2015). This elucidates the tendency toward viability-based modelling and the insufficiency of crude mechanisms like genetic algorithms in capturing the creative potency of living systems. Cybernetic mimesis, like lateral gene transfer, is a mechanism for horizontal transmission of representational structure between agents---not vertical inheritance through selection, but direct exchange and transformation of symbolic material. In this picture, the old opposition between Lamarckism and Darwinism dissolves. External selection (Darwinian) and internal selection (Lamarckian, in the sense of organism-generated directional bias) are not competing explanations but complementary faces of the same developmental-evolutionary process. What Lamarck got right---that organisms actively shape their own trajectory---is recovered not through inheritance of acquired characters but through the interiorization-exteriorization loop operating across developmental and evolutionary timescales.

\subsection{The Problem of Meaning}

Underneath this is the problem of meaning. The parts of a whole can only have meaning with regard to how they relate to the functioning of the whole, and this functioning is of a pragmatic nature---it is what the system \textit{does}, not what it \textit{is made of}. Meaning requires a context of function. This also illuminates the tendency toward simplistic Darwinian reductionism: survival is indeed a pragmatic function, and in that sense Darwinism correctly identifies pragmatics as fundamental. But survival is an \textit{externally} defined function---defined by environmental fitness, not by the organism itself. as such, it cannot entail the inherent meaningfulness nor agency of the organism. An organism whose only function is survival-as-defined-by-the-environment is a channel for external forces, not a meaning generating autonomous agent. Genuine agency requires \textit{internally} defined pragmatics---functionality specified by the organism's own developmental and operational structure. This is what Vitality provides. Vitality as intrinsic motivation---the urge to increase both controllability and sensitivity to the environment---gives us an internally defined pragmatics and hence first-person meaning, for it is internally specified. What matters to the organism is not dictated by an external fitness function but by its own E--P dynamics: expand your influence, deepen your responsiveness, and let the two co-regulate. This is first-person meaning---and first-person meaning is the only kind of meaning there is. Meaning that is externally assigned is not meaning but labelling.

\subsection{Non-Computationality: Development, Not Optimization}

There are arguments that the non-computational nature of organisms is tied to the intractability of formalizing the environmental fitness landscape---that computational complexity is an ultimate constraint on evolution, preventing populations from reaching fitness optima within feasible timescales (Kaznatcheev 2019), and that organisms navigate ``large worlds'' of unprecedented situations that cannot be precoded algorithmically (Jaeger et al.\ 2024). Though correct, this diagnosis betrays an insistence on the survival and viability framing. It does not acknowledge the intrinsic selection and recombination of representations---affordances---which, driven by the intrinsic function of Vitality, creates recombinations of perceptual schemas and projections that could not arise through mere optimization of input perceptions. The oscillating Vitality irreversibly alters the trajectory of the representation learning apparatus from whatever its convergence would otherwise have been. The autonomous nature of the oscillator and the intrinsically motivated drive as constraint create projections, resulting behaviors, and refinements of representations that can only be understood as development, not mere optimization. The organism is non-computational not because the fitness landscape is intractable, but because development under Vitality is a qualitatively different process from optimization---one that generates its own problem space rather than solving a given one.

%% ----------------------------------------------------------------
\section{Vitality: Formalizing Play}

\subsection{Setup}

We work with an agent whose state space $\Xspace$ is partitioned into internal state $Y_t$ and world state $W_t$. The agent interacts through an interface $\lambda$ consisting of actions $A_{1:T}$ and observations $O_{1:T}$. Policies $\pi$ induce trajectory distributions over this joint space.

\subsection{Formalizing Bidirectional Engagement}

\begin{definition}[Empowerment]
\[
  E_t(\lambda) := I(A \leadsto O;\, \lambda)_t
\]
Channel capacity for actions to influence future observations.
\end{definition}

\begin{definition}[Plasticity]
\[
  P_t(\lambda) := I(O \leadsto A;\, \lambda)_t
\]
Channel capacity for observations to reshape future actions.
\end{definition}

\begin{proposition}[Abel Tradeoff]
$E_t + P_t \leq m_t(\lambda)$, where $m_t(\lambda)$ is interface-dependent upper bound.
\end{proposition}

\begin{figure}[ht]
\centering
\includegraphics[width=0.85\textwidth]{figures/abel_tradeoff.png}
\caption{Interface capacities $E_t(\lambda)$ (empowerment) and $P_t(\lambda)$ (plasticity) as mirror quantities (left), and the tight upper bound on their joint values (right). Values of $E_t(\lambda)$ and $P_t(\lambda)$ from zero up to an interface- and interval-dependent constant $m_t(\lambda)$ are realizable, but their sum can be no greater than $m_t(\lambda)$, illustrating Abel's bidirectional tradeoff geometry.}
\label{fig:abel-tradeoff}
\end{figure}

\textbf{Vitality as Co-Regulatory Regime.} Vitality is the Turing-esque dynamical pattern where empowerment and plasticity co-regulate each other: E regulates P (influencing the world shapes what reconfigures you), P regulates E (reconfigurability sustains influence in non-stationary environments). This is a specific behavioral regime, not just a number.

\begin{definition}[Vitality Measure (time-local)]
\[
  V_t(\lambda) := \frac{2\,E_t\, P_t}{E_t + P_t}
\]
This harmonic mean measures the strength of the co-regulatory regime at time $t$. It requires both E and P positive, penalizes extremes, and reaches maximum when $E_t = P_t$.
\end{definition}

\subsection{Temporal Morphogens}

\begin{definition}[Internalization]
Interface capacities (fast measurements) internalized as slow state:
\[
  \Ecal_{t+1} = (1 - \alpha_E)\Ecal_t + \alpha_E\, E_t(\lambda), \qquad
  \Pcal_{t+1} = (1 - \alpha_P)\Pcal_t + \alpha_P\, P_t(\lambda)
\]
\end{definition}

Here $\Ecal_t$ is a slow ``skill/commitment'' morphogen and $\Pcal_t$ is a slow ``reconfigurability'' morphogen.

\textbf{Knowledge as self-persisting information.} The morphogens are not merely statistics about past interaction---they are \textit{knowledge} in the constructor-theoretic sense (Deutsch 2013, Marletto 2015). Constructor theory defines knowledge as information that causes itself to remain instantiated---information whose effects on the world tend to bring about its own persistence. This is exactly the structure of the morphogens. $\Ecal_t$ and $\Pcal_t$ are the agent's consolidated developmental history, actively maintaining themselves through the E--P loop. They persist not because they are written to memory, but because they regulate the very interactions that regenerate them. The morphogens ARE what the agent has learned, and they persist precisely because they shape how the agent continues to engage with the world. This is Piaget's constructionism recast in constructor-theoretic terms: knowledge is not stored data but self-maintaining structure.

\textbf{Structure.} The morphogens modulate the fast loop: adaptation strength is proportional to $f(\Pcal_t)$ while stability and precision are proportional to $g(\Ecal_t)$. This creates a reaction--diffusion structure where $\Ecal$ acts as activator (stabilizing) and $\Pcal$ acts as inhibitor (destabilizing). The different timescales---$\alpha_P \ll \alpha_E$, making plasticity slow to recruit but fast to act---produce temporal Turing patterns: alternating plateau phases (exploit, high $\Ecal$, low $\Pcal$) and burst phases (explore, recruited $\Pcal$). Developmental transitions emerge as bifurcations between behavioral regimes.

\subsection{Formalizing Symbolic Abstraction}

Given observations $\{o_t\}$ from interaction, we learn an encoder $\phi: \Ospace \to \Zspace$ mapping into a factored latent space $\Zspace = \Zspace_1 \times \cdots \times \Zspace_k$. The requirements are threefold: (1)~\textbf{group-theoretic decomposition}, where each $\Zspace_i$ transforms independently under a subgroup $G_i$; (2)~\textbf{transformation invariance}, where each factor captures a structural invariant reusable across contexts; and (3)~\textbf{discretizability}, where factors are naturally discretizable (finite groups, discrete orbits), yielding a vocabulary of \textit{perceptual symbols}.

The algebraic structure is central. Each factor $\Zspace_i$ is not an arbitrary latent dimension but a representation space for a transformation group $G_i$. When the world transforms under $g \in G_i$ (rotation, translation, scaling), the corresponding latent factor transforms covariantly: $\phi_i(g \cdot o) = g \cdot \phi_i(o)$. This is equivariance: the representation respects the group action. The constraint learning objective biases the encoder to discover these group structures by factoring the full transformation group into conditionally independent subgroups. The result is a disentangled representation where each factor captures an independent degree of freedom in the world's causal structure.

Discretizability follows naturally from group structure. Finite groups (symmetries of discrete objects) yield finite vocabularies directly. Continuous groups (rotations, translations) discretize via orbit representatives: choose a canonical element from each equivalence class under the group action. For example, rotation-invariant features discretize by selecting canonical orientations, yielding symbols like ``vertical edge'' and ``horizontal edge'' rather than continuous angle values. This is not lossy quantization---it is extraction of the invariant. The symbol ``vertical edge'' is more primitive than ``edge at 87 degrees'' because it is stable under small perturbations and reusable across contexts.

Composability is the final requirement. Symbols must combine productively. If $\Zspace_i$ represents ``shape'' and $\Zspace_j$ represents ``color,'' the product space $\Zspace_i \times \Zspace_j$ represents ``colored shapes,'' and novel combinations (red triangle, blue square) are immediately available without retraining. This combinatorial productivity is what makes symbols different from distributed representations: a new percept involving known symbols is not out-of-distribution, it is a novel combination of known primitives.

\begin{definition}[Symbolic Inventory]
$\Sset_t = \{(\Zspace_i, G_i, \phi_i)\}_{i=1}^{k_t}$ grows over development. Each entry is a factored representation space, its transformation group, and an encoder. The inventory expands as new invariants are discovered.
\end{definition}

\subsection{The Integrated Definition}

\begin{definition}[Vitality (Version 3)]
A system exhibits Vitality over horizon $T$ if it sustains the E--P co-regulatory regime with the following properties:
\begin{enumerate}
  \item \textbf{High dynamic engagement}: $\bar{V}_T$ sustained above threshold, indicating strong bidirectional coupling between empowerment and plasticity.
  \item \textbf{Growing symbolic inventory}: $|\Sset_T| > |\Sset_0|$ (reusable abstractions extracted from the interaction data generated by E--P oscillation).
  \item \textbf{Loop closure}: learned symbols become new E--P substrates, so empowerment--plasticity dynamics shift to operate over symbolic space, not just raw sensorimotor channels.
\end{enumerate}
\end{definition}

The distinguishing feature is condition~(3). This is not simply ``intrinsic motivation + representation learning.'' The loop closes: symbols are recruited into control, which generates new interactions, which extract further abstractions.

\subsection{Pathologies}

\begin{center}
\begin{tabular}{@{}lp{9cm}@{}}
\toprule
\textbf{Pathology} & \textbf{Why excluded} \\
\midrule
Frozen agent & $\Pcal_t \to 0 \Rightarrow V_t \to 0$ (no negative feedback) \\
Rewrite-soup & $\Ecal_t \to 0 \Rightarrow V_t \to 0$ (no positive feedback) \\
Novelty chaser & No symbolic extraction despite exploration \\
Compressor & No bidirectional engagement driving feature selection \\
Static optimizer & No projection of symbols back into E--P---development stops \\
\bottomrule
\end{tabular}
\end{center}

%% ----------------------------------------------------------------
\section{Best Mechanisms We Have}

\subsection{For Bidirectional Engagement}

Abel et al.\ (2025) formalize plasticity as the dual of empowerment within generalized directed information. They define channel capacities in both directions---forward ($A \to O$) and reverse ($O \to A$)---and prove a tight tradeoff: $E + P \leq m(\lambda)$. The interface has finite bidirectional capacity, and any allocation must respect this bound.

Internalizing E and P as slow morphogens yields reaction--diffusion structure. The plateau phase (high $\Ecal$, low $\Pcal$) corresponds to exploitation. The burst phase (recruited $\Pcal$) corresponds to exploration. This maps directly onto the explore--exploit oscillation familiar from reinforcement learning, but here it emerges from the coupled morphogen dynamics rather than being imposed externally. Developmental transitions emerge as bifurcations in this dynamical system.

\subsection{For Symbolic Abstraction}

The algebraic constraint learning framework (Nishitsunoi et al.\ 2025) provides the theoretical foundation. It biases feature discovery toward group-theoretic decompositions by imposing equivariance constraints during learning. The key insight is that the world's transformations factor into conditionally independent subgroups, and a well-structured representation should mirror this factorization. The learning objective encourages the encoder to discover these subgroups: when a transformation $g$ acts on observations, the encoder should decompose $g$ into independent components acting on separate latent factors. This is not merely disentanglement (statistical independence of latents) but algebraic disentanglement (factorization of the transformation group). The result is features that are operationally meaningful (each factor corresponds to an independent degree of freedom), naturally discretizable (via orbit representatives), and composable (the product structure enables novel combinations).

The implementation challenge is that group structure is not given a priori---it must be discovered from data. The constraint learning framework addresses this by learning both the encoder and the group factorization jointly. The encoder proposes candidate factors, and the factorization objective measures how cleanly transformations decompose across these factors. Features that fail to respect the group structure are penalized. Over development, this produces an inventory of symbols that are not arbitrary latent codes but genuine structural invariants.

Discrete codebook methods provide complementary techniques for enforcing discreteness. VQ-VAE (van den Oord et al.\ 2017) constructs discrete latent representations via vector quantization: the continuous encoder output is mapped to the nearest entry in a learned discrete codebook. This enforces a finite vocabulary while preserving gradient-based learning through the straight-through estimator. Discrete Codebook World Models (Scannell et al.\ 2025) extend this by preserving ordinal relationships: the codebook is structured so that perceptually similar observations map to nearby codebook entries. This combines discreteness (finite vocabulary, combinatorial recombination) with metric structure (similarity is preserved). The result is a symbolic representation that supports both compositional reasoning and smooth generalization.

GFlowNets (Bengio et al.\ 2023) address a different aspect of symbolic abstraction: navigating discrete combinatorial structure. Traditional neural methods struggle with discrete search spaces because gradients do not propagate through discrete choices. GFlowNets treat discrete generation as a stochastic flow, where the probability of constructing an object is proportional to a learned reward. This enables flexible exploration of symbolic spaces---searching over discrete programs, molecular graphs, or compositional symbolic structures---with neural guidance. In the Vitality context, GFlowNets could enable symbolic search over candidate representations: exploring the combinatorial space of factorizations to find structures that maximize $V$.

DreamCoder (Ellis et al.\ 2021) demonstrates how symbolic inventories can grow through use. It bootstraps compositional program libraries through wake-sleep cycles: during the wake phase, the system solves tasks by composing existing primitives; during the sleep phase, it abstracts common patterns into new reusable primitives, which are then added to the library. The library grows over time, and later tasks are solved by composing high-level abstractions rather than low-level operations. This is a computational model of loop closure: symbols extracted from problem-solving (wake) become primitives for future problem-solving (sleep), and the cycle repeats. Its discrete, composable building blocks---programs in a typed lambda calculus---provide a concrete instantiation of what a symbolic inventory looks like when it expands developmentally.

\subsection{For the Integration}

The integration of dynamic engagement with symbolic abstraction is the core novelty. Current representation learning and intrinsic motivation frameworks fail because they address one side without the other. Representation learning produces features without knowing which are behaviorally useful. Intrinsic motivation explores without extracting reusable abstractions. Neither closes the developmental loop. Vitality integrates them: E--P selects which features matter, features become new E--P substrates, and the cycle repeats. This section makes the integration mechanism explicit.

\subsubsection{E--P as Selection Pressure for Representation Learning}

Representation learning suffers from an underdetermination problem: in unsupervised settings, infinitely many representations compress the data equally well. There is no principled selection criterion beyond reconstruction error or contrastive loss. The result is that learned features may be statistically optimal but behaviorally useless. A representation that perfectly reconstructs pixels might miss the affordances that matter for action. A representation that maximizes mutual information between temporally adjacent frames might encode irrelevant details while ignoring controllable structure.

E--P provides the missing selection pressure. A feature $z$ is valuable if and only if it expands both $E$ (controllability via $z$) and $P$ (sensitivity to $z$). This is a functional criterion: features must support bidirectional coupling. It filters out features that compress well but don't enable better engagement. Concretely, a feature that captures static visual texture (high reconstruction quality) but is uncontrollable (low $E$) will not be selected. A feature that captures a perfectly controllable variable (high $E$) but is insensitive to environmental changes (low $P$) will also be rejected. Only features that enable both influence and responsiveness---bidirectional engagement---are retained.

The formal mechanism is straightforward. At time $t$, the agent has symbolic inventory $\Sset_t = \{z_1, \ldots, z_k\}$. For each symbol $z \in \Sset_t$, compute $E_t(\lambda_z)$ and $P_t(\lambda_z)$---the empowerment and plasticity available when controlling or observing through $z$. Features with high vitality $V_t(\lambda_z) = 2E_t(\lambda_z)P_t(\lambda_z)/(E_t(\lambda_z)+P_t(\lambda_z))$ are retained and elaborated. Features with low $V_t(\lambda_z)$ are pruned or not extracted in the first place. This implements developmental filtering: the dynamic layer shapes what the symbolic layer learns.

The practical implication is that representation learning should be constitutively constrained by E--P expansion. The training objective should not be reconstruction or contrastive prediction alone, but E--P expansion through the learned features. This inverts the standard pipeline: instead of learning representations first and then using them for control, the control objective (bidirectional engagement) shapes representation learning from the start. Features emerge because they expand the agent's capacity for play, not because they compress data efficiently.

\subsubsection{Symbols as New E--P Substrates}

Once extracted, symbols $z \in \Sset_t$ become new control and observation targets. The E--P dynamics shift from sensorimotor space $(A, O)$ to symbolic space $(Z_A, Z_O)$. The agent now seeks to influence and be influenced at the level of abstractions, not raw percepts.

A Piaget-style example clarifies this. At the sensorimotor stage, E--P operates over raw actions (grasp, release) and observations (visual flow, tactile feedback). At the object stage, the agent abstracts ``graspable object,'' and E--P now operates over object-level actions (``grab the cup'') and object-level observations (``cup moved''). At the relational stage, the agent abstracts ``containment,'' and E--P operates over relational actions (``put A in B'') and relational observations (``A is inside B''). Each level's symbols become the substrate for the next level's E--P oscillation.

This closes the loop. Symbols extracted from E--P data at level $n$ become E--P targets at level $n+1$. This is recursive abstraction: the mechanism that generates symbols also consumes them. Contrast this with static representation learning, where features are extracted once and used for downstream tasks but not fed back into the feature-extraction process itself.

\subsubsection{Hierarchical Developmental Dynamics}

The developmental loop does not merely iterate---it builds hierarchical structure. This connects to two foundational frameworks: Powers' Perceptual Control Theory (PCT), which describes behavior as a hierarchy of feedback control loops at increasing levels of abstraction, and Piaget's developmental stages, which document how children construct increasingly abstract operational capacities through interaction.

Powers (1973) proposed that control is hierarchically organized: lower levels control immediate perceptual intensities, while higher levels control relationships, sequences, principles, and systematic goals. Each level sets reference signals for the level below, and each level perceives through the outputs of lower levels. The hierarchy is not imposed externally---it emerges from the recursive composition of control loops. Piaget (1952, 1954) documented the developmental progression empirically: sensorimotor intelligence grounds in direct causal interaction, preoperational thought introduces symbolic representation, concrete operations enable reversible transformations and conservation, and formal operations abstract over operations themselves.

E--P plus symbolic abstraction mechanizes this progression. The ground floor is causal. At the sensorimotor level, E--P oscillates over raw actions (reach, grasp, push) and observations (visual motion, tactile contact, proprioceptive feedback). The agent probes: \textit{when I push here, what happens?} High E--P engagement generates rich interaction data exhibiting causal structure. This is Powers' intensity and sensation control: the most basic feedback loops, closing through the physics of the body-world coupling. Algebraic constraint learning extracts transformation-invariant features from this data: ``graspable object,'' ``containment,'' ``occlusion.'' These are not arbitrary compressions---they are features that expand $V$ by enabling finer control (empowerment) and richer response (plasticity).

Once ``object'' is extracted, it becomes a new control target. The conscious layer's index sequences reconfigure to address objects as units. The E--P dynamics shift upward: the agent no longer seeks to control raw visual flow, but to control objects as persistent, manipulable entities. High-$V$ regions now involve object-level interactions: ``retrieve occluded object,'' ``stack blocks.'' This is Piaget's transition from sensorimotor to preoperational intelligence, and Powers' transition to configuration and transition control. The data generated at this level exhibits relational structure. The constraint learner extracts ``containment,'' ``support,'' ``behind/in-front'' as the next symbolic layer.

At the relational level, E--P operates over relations. The agent seeks to control \textit{which relations hold}: ``make A contain B,'' ``put C on top of D.'' This generates data exhibiting compositional structure: relations compose (A on B, B on C implies A above C). The constraint learner extracts transformation rules: commutativity, associativity, conservation. These are Piaget's concrete operations and Powers' relationship and category control.

At the formal level, E--P operates over transformation rules themselves. The agent seeks to control \textit{which operations are valid}: ``find the set of moves that solves this puzzle,'' ``construct a proof.'' This generates data exhibiting algebraic structure: groups, morphisms, equivalence classes. The constraint learner extracts metasymbolic features: ``symmetry,'' ``invariance,'' ``homomorphism.''

Each transition is driven by the same mechanism: E--P oscillation at level $\ell$ generates interaction data, algebraic constraint learning extracts symbols $\Sset_{\ell+1}$ that are transformation-invariant at level $\ell$, and these symbols become new control targets at level $\ell+1$. The hierarchy is not hand-designed. It emerges because symbols extracted from causal interaction are inherently compositional, and composition generates structure that itself becomes worth controlling.

The key insight is that new control levels are not pre-wired but emerge through E--P-driven abstraction. This produces an emergent curriculum: the agent naturally explores near its competence boundary---the region where $V$ is high but not yet saturated. High-$V$ regions become new training grounds. Developmental transitions are bifurcations in the coupled E--P--symbolic system: as new symbols are recruited, the dynamics qualitatively shift to operate at a higher level. No external curriculum is needed---the loop generates its own scaffolding.

\begin{proposition}[Integration as Core Novelty]
Neither intrinsic motivation alone (empowerment without symbolic extraction) nor representation learning alone (symbolic extraction without intrinsic selection pressure) produces open-ended development. The integration---E--P selects features, features become new E--P substrates---closes the developmental loop.
\end{proposition}

%% ----------------------------------------------------------------
\section{Problems with These Mechanisms}

The mechanisms in Section~5 are the best we have. They are also structurally limited in ways that define the research agenda.

\subsection{The Entropic Problem}

E and P are mutual information quantities. $V_t$ inherits every limitation of information-theoretic optimization. These are not engineering complaints---they are theorem-level structural limitations. This diagnosis is not unique to Vitality. The broader critical literature on entropy-based frameworks has reached the same conclusion from multiple directions: Biehl et al.\ (2021) showed that key derivation steps in the Free Energy Principle require unstated assumptions and proved the original ``free energy lemma'' wrong by counterexample; Andrews (2021) argued that the FEP has no explanatory content beyond specific construals; Bruineberg et al.\ (2022) demonstrated that the FEP equivocates between statistical and ontological interpretations of Markov blankets. The problem is not with one particular framework---it is with using entropy as the highest-level objective for autonomous cognition.

\textbf{Problem 1: Representation presupposition (Shannon 1949).} Computing $I(A \leadsto O)$ requires well-defined random variables and probability spaces. But choosing the right variables is the hard problem. Shannon explicitly excluded semantics from information theory---``the semantic aspects of communication are irrelevant to the engineering problem'' (Shannon \& Weaver 1949). Bickhard (1995) calls this \textit{encodingism}: the presupposition that representation has the nature of encodings, which is circular because encodings have content only by virtue of an encoding-user who already knows that content. Dretske (1981) attempted to ground cognition in information theory and failed---his account could not handle misrepresentation, a fatal flaw for any theory of cognition. The consequence is that information theory assumes the representation problem is already solved. V3's algebraic learner partially addresses this by providing variables, but the bootstrap remains: initial E--P must operate over raw sensorimotor variables whose structure is assumed.

\textbf{Problem 2: Intractability (McAllester \& Stratos 2020, Theorem~1).} Any distribution-free lower bound on MI estimated from $N$ samples cannot be larger than $O(\ln N)$. The number of samples required is exponential in the true MI. Variational bounds do not save you: Poole et al.\ (2019) showed that variational lower bounds on MI degrade when MI is large---exhibiting either high bias or high variance. Song \& Ermon (2020) proved that estimators like MINE exhibit variance that grows exponentially with true MI, and that existing estimators fail to satisfy basic self-consistency properties such as the data processing inequality and additivity under independence. For small discrete codebooks this softens (finite sums, Blahut-Arimoto becomes tractable), but $n$-step empowerment scales as $|A|^n$, and compositional state spaces explode combinatorially. These are proven limitations, not solvable optimization problems.

\textbf{Problem 3: Structural poverty.} Entropy collapses all structure into a scalar. It cannot distinguish algebraic from topological from causal constraints. $V_t = 2EP/(E+P)$ is a single number, blind to the compositional structure of $\Sset_t$ that it supposedly governs. The harmonic mean does not know about the group theory of the symbolic inventory. This matters because the organizational properties that distinguish living from non-living systems are irreducibly structural. Moreno \& Mossio (2015) show that biological systems operate under two distinct causal regimes---thermodynamic processes and organizational constraints---and that autonomous systems exhibit \textit{closure of constraints}: a topological and algebraic property, not a scalar one. Mont\'evil \& Mossio (2015) formalize this as a set of internally produced, mutually dependent constraints that cannot be collapsed into a single entropy measure. The kind of self-maintaining organizational structure that Vitality aims to capture is precisely what entropy-as-scalar cannot express.

\textbf{Problem 4: Causal blindness (Bareinboim et al.\ 2022, Causal Hierarchy Theorem).} Mutual information is symmetric by definition: $I(X;Y) = I(Y;X)$. But causation is directional. The Causal Hierarchy Theorem states that observational data underdetermines interventional and counterfactual quantities. E and P are Layer~1 (associational)---they can be inflated by confounders and distributional drift. Genuine influence and sensitivity require interventional structure, which MI does not capture.

\subsection{The Self-Reinforcing Trap}

The trap has two exits, both blocked. Without symbols, you are stuck with entropy---your only objectives are statistical. Without non-entropic objectives, there is no pressure to learn symbols---continuous approximations are always locally sufficient. The subsymbolic-entropic paradigm is self-reinforcing: it defines both representation and objective in terms that exclude the structures that would reveal its limits.

The evidence for the first exit being blocked is substantial. Fodor \& Pylyshyn (1988) identified three properties of cognition---systematicity, productivity, and compositionality---that require combinatorial syntactic structure and remain substantially unmet by connectionist approaches after nearly four decades. Lake et al.\ (2017) identified compositionality, causal models, and intuitive physics as fundamentally missing from deep learning. Greff et al.\ (2020) showed that neural networks' inability to dynamically bind information affects their capacity for compositional understanding. Battaglia et al.\ (2018)---from within the deep learning community---argued explicitly that combinatorial generalization requires structured representations. The concrete failures are vivid: the reversal curse (Berglund et al.\ 2023), where LLMs trained on ``A is B'' systematically fail to learn ``B is A''; the SCAN benchmark (Lake \& Baroni 2018), where accuracy drops to near zero on novel compositions of known primitives; and Marcus's (2018) catalogue of deep learning's structural limitations. Without discrete combinatorial structure, the generative capacity that higher cognition requires is absent.

V3 attacks both exits. E--P provides the dynamics, algebraic constraints provide the symbols. But the tension between the entropic core (E and P are mutual information) and the algebraic periphery ($\Sset_t$ has non-entropic structure) remains unresolved. The harmonic mean does not know about the group theory. This is not a bug. It points toward what must be built next.

%% ----------------------------------------------------------------
\section{What Needs to Be Done}

The problems in Section~6 are real and foundational, not engineering complaints. Here is what must be built.

\subsection{A Causal, Computationally Tractable Bidirectional Intrinsic Motivation}

E--P gets the structure right: bidirectional, oscillatory, self-regulating plus self-amplifying. But the current realization in mutual information is both causally blind and intractable at scale.

The requirements for a successor are clear. It must be causal: grounded in interventional quantities ($P(O|\text{do}(A))$ not $P(O|A)$), so that influence and sensitivity are genuine rather than confounded. And it must be computationally tractable: not requiring exponential samples or variational bounds that change qualitative behavior.

Some machinery for a causal upgrade exists. Ay \& Polani (2008) defined causal information flow using Pearl's do-operator, and Simoes et al.\ (2024) defined causal entropy and causal information gain. Cao et al.\ (2025) demonstrated empowerment through causal structure learning in model-based RL, and Yiu et al.\ (2025) proposed a bidirectional link between accurate causal models and empowerment from the cognitive science side. But all of these remain fundamentally information-theoretic---they import causal structure from outside (DAGs, do-calculus, interventional distributions) while the information theory provides the quantification. The causal structure does the explanatory work; the entropy just counts the bits. An interventional upgrade---replacing $P(O|A)$ with $P(O|\text{do}(A))$---would address Problem~4 (causal blindness) but not Problems~1--3. Even with perfect causal identification, you are still maximizing a scalar mutual information quantity that presupposes its own variables, requires exponential samples, and collapses algebraic structure into bits. The deeper question is whether \textit{bits} are the right unit at all.

A complementary direction is Gunji and Pegios, ``Natural-Born Intelligence as the Invocation of Emotion = Politics,'' which connects bidirectional engagement to non-information-theoretic quantities that might sidestep the MI trap entirely.

\subsection{Representation Learning That Does Both Indexing and Feature Extraction}

The formal criteria for symbolic abstraction laid out in \S4.4 specify exactly what a representation learning system must deliver: (1) \textbf{group-theoretic decomposition}---factoring the latent space into independent subspaces, each transforming under its own subgroup; (2) \textbf{transformation invariance}---each factor capturing a structural invariant stable across contexts, not a transient statistical regularity; (3) \textbf{discretizability}---factors that naturally discretize via finite group orbits or canonical representatives, yielding a vocabulary of perceptual symbols rather than continuous vectors; and (4) \textbf{composability}---symbols that combine productively in product spaces, so novel combinations of known primitives are immediately available without retraining.

No existing method satisfies all four. VQ-VAE indexes---it builds a discrete codebook---but the codebook entries have no algebraic structure. They are arbitrary centroids in latent space, not group-equivariant factors. Nishitsunoi et al.'s algebraic constraint learning satisfies (1) and (2)---it discovers conditionally independent transformation groups and learns equivariant encoders---but produces continuous representations without an indexed vocabulary. DreamCoder achieves (3) and (4) in the symbolic domain---discrete, composable library primitives---but operates over programs, not perceptual features, and has no mechanism for grounding symbols in sensorimotor interaction.

What's needed is a system that builds a discrete inventory of structurally meaningful features where the indexing and the algebraic structure are the same thing: each symbol in the codebook corresponds to an orbit under a learned transformation group, discretized via canonical representatives, and composable with other symbols through the product structure of the factored latent space. The indexing \emph{is} the group-theoretic decomposition, not a separate quantization step bolted on afterward.

\subsection{Integration: Representation Learning Constrained by the Mother-of-All-Values Intrinsic Motivation}

This is the deepest open problem. The dynamic engagement component---E--P, or its causal successor---should function as a higher-order constraint on representation learning, not just supplying selection pressure for features but determining what kind of representations are worth having in the first place.

Vitality is the mother of all values, the intrinsic ``necessity'' that all representation and action ascend towards. Features are valuable not because they compress data, not because they reconstruct observations, but because they expand the agent's capacity for bidirectional engagement with the world.

Representation learning should be constitutively constrained by E--P expansion. The objective for feature extraction should not be reconstruction or contrastive prediction, but E--P expansion itself. The symbolic and dynamic components are not just coupled---they are aspects of a single process. The agent simultaneously learns what to attend to and how to engage with what it attends to.

We return to the opening epigraph. The creative mind plays with the objects it loves---and now we have a mechanization of that love. Empowerment is \textit{eros}: the outward drive to shape, to influence, to impress oneself upon the world. Plasticity is \textit{agape}: the receptive openness to be shaped, to be affected, to let the world in. These are the two canonized \textit{coincidentia oppositorum} of love---active and passive, reaching out and taking in---and their oscillation is Vitality. The creative agent does not merely optimize over the world. It plays with the world, which is to say it loves the world: identifying with what it engages, and being transformed by what it identifies with. This is the play instinct acting from inner necessity. This is Vitality.

%% ----------------------------------------------------------------

\begin{thebibliography}{99}

\bibitem{abel2025} Abel, D., et al.\ (2025). Plasticity as the Mirror of Empowerment. \textit{arXiv:2505.10361}.

\bibitem{andrews2021} Andrews, M.\ (2021). The Math is Not the Territory. \textit{Biology \& Philosophy}, 36(3).

\bibitem{ay2008} Ay, N.\ \& Polani, D.\ (2008). Information Flows in Causal Networks. \textit{Advances in Complex Systems}, 11(1).

\bibitem{bareinboim2022} Bareinboim, E., et al.\ (2022). On Pearl's Hierarchy and the Foundations of Causal Inference. \textit{ACM Books}.

\bibitem{battaglia2018} Battaglia, P.W., et al.\ (2018). Relational Inductive Biases, Deep Learning, and Graph Networks. \textit{arXiv:1806.01261}.

\bibitem{bengio2023} Bengio, E., et al.\ (2023). GFlowNet Foundations. \textit{JMLR}, 24.

\bibitem{berglund2023} Berglund, L., et al.\ (2023). The Reversal Curse: LLMs Trained on ``A is B'' Fail to Learn ``B is A.'' \textit{arXiv:2309.12288}.

\bibitem{bickhard1995} Bickhard, M.H.\ (1995). \textit{Foundational Issues in Artificial Intelligence and Cognitive Science}. John Benjamins.

\bibitem{biehl2021} Biehl, M., Pollock, F.A.\ \& Kanai, R.\ (2021). A Technical Critique of Some Parts of the Free Energy Principle. \textit{Entropy}, 23(3).

\bibitem{bruineberg2022} Bruineberg, J., et al.\ (2022). The Emperor's New Markov Blankets. \textit{Behavioral and Brain Sciences}, 45.

\bibitem{cao2025} Cao, J., et al.\ (2025). Towards Empowerment Gain through Causal Structure Learning in Model-Based RL. \textit{arXiv:2502.10077}.

\bibitem{deacon2005} Deacon, T.W.\ (2005). Beyond Piaget's Phenocopy: The Baby in the Lamarckian Bath. In S.T.\ Parker, J.\ Langer \& C.\ Milbrath (Eds.), \textit{Biology and Knowledge Revisited: From Neurogenesis to Psychogenesis}. Routledge.

\bibitem{deutsch2013} Deutsch, D.\ (2013). Constructor Theory. \textit{Synthese}, 190(18).

\bibitem{dretske1981} Dretske, F.\ (1981). \textit{Knowledge and the Flow of Information}. MIT Press.

\bibitem{ellis2021} Ellis, K., et al.\ (2021). DreamCoder. \textit{PLDI}.

\bibitem{fodor1988} Fodor, J.A.\ \& Pylyshyn, Z.W.\ (1988). Connectionism and Cognitive Architecture: A Critical Analysis. \textit{Cognition}, 28.

\bibitem{greff2020} Greff, K., van Steenkiste, S.\ \& Schmidhuber, J.\ (2020). On the Binding Problem in Artificial Neural Networks. \textit{arXiv:2012.05208}.

\bibitem{gunji} Gunji, Y.-P.\ \& Pegios, A.\ Natural-Born Intelligence as the Invocation of Emotion = Politics.

\bibitem{jaeger2024} Jaeger, J., Riedl, A., Djedovic, A., Vervaeke, J.\ \& Walsh, D.\ (2024). Naturalizing Relevance Realization: Why Agency and Cognition Are Fundamentally Not Computational. \textit{Frontiers in Psychology}, 15.

\bibitem{jung1967} Jung, C.G.\ (1967). \textit{Alchemical Studies}. Collected Works, Vol.\ 13. Bollingen Series XX. Princeton University Press.

\bibitem{kaznatcheev2019} Kaznatcheev, A.\ (2019). Computational Complexity as an Ultimate Constraint on Evolution. \textit{Genetics}, 212(1).

\bibitem{klopf1982} Klopf, A.H.\ (1982). \textit{The Hedonistic Neuron: A Theory of Memory, Learning, and Intelligence}. Hemisphere.

\bibitem{lake2017} Lake, B.M., Ullman, T.D., Tenenbaum, J.B.\ \& Gershman, S.J.\ (2017). Building Machines That Learn and Think Like People. \textit{Behavioral and Brain Sciences}, 40.

\bibitem{lake2018} Lake, B.M.\ \& Baroni, M.\ (2018). Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks. \textit{arXiv:1711.00350}.

\bibitem{marcus2018} Marcus, G.\ (2018). Deep Learning: A Critical Appraisal. \textit{arXiv:1801.00631}.

\bibitem{marletto2015} Marletto, C.\ (2015). Constructor Theory of Life. \textit{Journal of the Royal Society Interface}, 12(104).

\bibitem{mcallester2020} McAllester, D.\ \& Stratos, K.\ (2020). Formal Limitations on the Measurement of Mutual Information. \textit{AISTATS}.

\bibitem{montevil2015} Mont\'evil, M.\ \& Mossio, M.\ (2015). Biological Organisation as Closure of Constraints. \textit{Journal of Theoretical Biology}, 372.

\bibitem{moreno2015} Moreno, A.\ \& Mossio, M.\ (2015). \textit{Biological Autonomy: A Philosophical and Theoretical Enquiry}. Springer.

\bibitem{nishitsunoi2025} Nishitsunoi, K., Ohmura, Y., Komatsu, T.\ \& Kuniyoshi, Y.\ (2025). Learning Conditionally Independent Transformations using Normal Subgroups in Group Theory. \textit{arXiv:2504.04490}.

\bibitem{panksepp1998} Panksepp, J.\ (1998). \textit{Affective Neuroscience}. Oxford University Press.

\bibitem{pearl2009} Pearl, J.\ (2009). \textit{Causality} (2nd ed.). Cambridge University Press.

\bibitem{piaget1952} Piaget, J.\ (1952). \textit{The Origins of Intelligence in Children}. International Universities Press.

\bibitem{piaget1954} Piaget, J.\ (1954). \textit{The Construction of Reality in the Child}. Basic Books.

\bibitem{poole2019} Poole, B., Ozair, S., Van Den Oord, A., Alemi, A.\ \& Tucker, G.\ (2019). On Variational Bounds of Mutual Information. \textit{ICML}.

\bibitem{powers1973} Powers, W.T.\ (1973). \textit{Behavior: The Control of Perception}. Aldine.

\bibitem{roachford2023} Roachford, T.\ (2023). Ensoul: A Framework for the Creation of Self Organizing Intelligent Ultra Low Power Systems (SOULS) through Evolutionary Enerstatic Networks. \textit{arXiv:2304.13863}.

\bibitem{scannell2025} Scannell, A., et al.\ (2025). Discrete Codebook World Models. \textit{ICLR}.

\bibitem{sepulchre2022} Sepulchre, R.\ (2022). Control Across Scales. \textit{Annual Review of Control, Robotics, and Autonomous Systems}.

\bibitem{stern2010} Stern, D.N.\ (2010). \textit{Forms of Vitality: Exploring Dynamic Experience in Psychology and the Arts}. Oxford University Press.

\bibitem{shannon1949} Shannon, C.E.\ \& Weaver, W.\ (1949). \textit{The Mathematical Theory of Communication}. University of Illinois Press.

\bibitem{simoes2024} Simoes, F., et al.\ (2024). Fundamental Properties of Causal Entropy and Information Gain. \textit{PMLR}, 236.

\bibitem{song2020} Song, J.\ \& Ermon, S.\ (2020). Understanding the Limitations of Variational Mutual Information Estimators. \textit{ICLR}.

\bibitem{trevarthen1979} Trevarthen, C.\ (1979). Communication and Cooperation in Early Infancy. In \textit{Before Speech}. Cambridge University Press.

\bibitem{trevarthen1999} Trevarthen, C.\ (1999). Musicality and the Intrinsic Motive Pulse. \textit{Musicae Scientiae}, Special Issue, 155--215.

\bibitem{trevarthen2025} Trevarthen, C.\ (2025). In J.\ Delafield-Butt \& V.\ Reddy (Eds.), \textit{Intersubjective Minds: Rhythm, Sympathy, and Human Being}. Oxford University Press.

\bibitem{vqvae2017} van den Oord, A., et al.\ (2017). Neural Discrete Representation Learning. \textit{NeurIPS}.

\bibitem{walsh2015} Walsh, D.M.\ (2015). \textit{Organisms, Agency, and Evolution}. Cambridge University Press.

\bibitem{yiu2025} Yiu, E., et al.\ (2025). Empowerment Gain and Causal Model Construction. \textit{Philosophical Transactions of the Royal Society A}.

\end{thebibliography}

\end{document}
